import streamlit as st
import pandas as pd
import os
import json
from openai import OpenAI
from collections import defaultdict
import logging # Import logging for better error tracking

# Configure logging for debugging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# OpenRouter APIé…ç½®
# It's best practice to use Streamlit secrets for API keys, especially in deployment.
# st.secrets['openrouter_key'] will fetch the key from .streamlit/secrets.toml
# For local testing, you can keep the direct assignment or use an environment variable.
openrouter_key = os.getenv("OPENROUTER_API_KEY", "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") # Fallback for local testing
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=openrouter_key
)

def test_openrouter_connection():
    """æµ‹è¯•OpenRouter APIè¿æ¥"""
    try:
        test_prompt = "è¯·å›å¤'APIè¿æ¥æ­£å¸¸'"
        response = client.chat.completions.create(
            model="qwen/qwen3-235b-a22b",
            messages=[{"role": "user", "content": test_prompt}],
            max_tokens=10,
            extra_headers={
                "HTTP-Referer": "http://localhost",
                "X-Title": "å·¥ä½œæ€»ç»“åˆ†æç³»ç»Ÿ"
            }
        )
        # Ensure the response content is handled as UTF-8
        response_content = response.choices[0].message.content
        if response_content and response_content.strip() == "APIè¿æ¥æ­£å¸¸":
            st.success("âœ… OpenRouter APIè¿æ¥æ­£å¸¸")
        else:
            st.warning(f"âš ï¸ APIè¿”å›å¼‚å¸¸æˆ–å†…å®¹ä¸åŒ¹é…: {response_content}")
            logging.warning(f"API connection test returned: {response_content}")
    except Exception as e:
        st.error(f"âŒ OpenRouter APIè¿æ¥å¤±è´¥: {str(e)}")
        logging.error(f"OpenRouter API connection failed: {e}", exc_info=True)

# Streamlitç•Œé¢é…ç½®ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
st.set_page_config(page_title="å·¥ä½œæ€»ç»“åˆ†æç³»ç»Ÿ", layout="wide")
st.title("ğŸ“Š å·¥ä½œæ—¥æŠ¥æ€»ç»“åˆ†æ")

# åˆå§‹åŒ–session_state
if 'report_df' not in st.session_state:
    st.session_state.report_df = None
if 'last_uploaded' not in st.session_state:
    st.session_state.last_uploaded = None

# Test API connection first
test_openrouter_connection()

@st.cache_data(show_spinner=False) # Cache the function result to avoid re-running unnecessarily
def extract_keywords_for_person(person_name: str, summaries: list) -> list:
    """ä½¿ç”¨OpenRouter LLMåˆ†æä¸€ä¸ªäººçš„æ‰€æœ‰å·¥ä½œæ€»ç»“"""
    try:
        # Ensure all summaries are strings and handle potential NaNs if they sneak in
        cleaned_summaries = [str(s) for s in summaries if pd.notna(s)]
        if not cleaned_summaries:
            logging.info(f"No valid summaries found for {person_name}.")
            return []

        combined_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(cleaned_summaries)])

        # Ensure the system prompt is clear about JSON format
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå·¥ä½œåˆ†æåŠ©æ‰‹ï¼Œè¯·åˆ†æä¸€ä¸ªäººä¸€æ®µæ—¶é—´çš„å·¥ä½œæ€»ç»“ã€‚
1. æå–ä¸»è¦å·¥ä½œä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡ç”¨3-5ä¸ªå­—çš„çŸ­è¯­æè¿°ã€‚
2. ç›¸ä¼¼ä»»åŠ¡åº”è¿›è¡Œåˆå¹¶ï¼ˆå¦‚'å¤„ç†å™ªå£°'ã€'å™ªå£°æŠ•è¯‰å¤„ç†'ã€'å™ªéŸ³æµ‹è¯•'ç»Ÿä¸€å½’ä¸º'å™ªå£°å¤„ç†'ï¼‰ã€‚
3. åˆå¹¶è§„åˆ™ï¼šç›¸åŒé¢†åŸŸçš„å·¥ä½œä½¿ç”¨æœ€ç®€çŸ­çš„æ ‡å‡†åŒ–è¡¨è¿°ã€‚
4. å¿…é¡»è¿”å›çº¯JSONæ ¼å¼ï¼Œç¤ºä¾‹ï¼š{'tasks': ['ä»»åŠ¡1','ä»»åŠ¡2']}ã€‚è¯·ç¡®ä¿JSONæ˜¯å®Œå…¨åˆæ³•çš„ï¼Œæ²¡æœ‰å¤šä½™çš„å­—ç¬¦æˆ–æ ¼å¼é”™è¯¯ã€‚"""

        # Using a more robust model for extraction if available and cost-effective.
        # Deepseek-coder-v2-instruct might be good for structured output, but qwen/qwen3-30b-a3b:free is what you used.
        # It's crucial that the model supports 'json_object' response format reliably.
        response = client.chat.completions.create(
            model="qwen/qwen3-30b-a3b:free", # Stick to your chosen model
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·åˆ†æ{person_name}çš„å·¥ä½œæ€»ç»“ï¼Œæå–æ ‡å‡†åŒ–çš„å·¥ä½œä»»åŠ¡:\n{combined_text}"}
            ],
            temperature=0.2,
            max_tokens=1000,
            response_format={"type": "json_object"}, # This is key for structured output
            extra_headers={
                "HTTP-Referer": "http://localhost",
                "X-Title": "å·¥ä½œæ€»ç»“åˆ†æç³»ç»Ÿ"
            }
        )
        result_content = response.choices[0].message.content
        logging.info(f"Raw LLM response for {person_name}: {result_content}")

        try:
            data = json.loads(result_content)
            if isinstance(data, dict) and 'tasks' in data and isinstance(data['tasks'], list):
                # Ensure tasks are strings and clean up whitespace
                return [task.strip() for task in data['tasks'] if isinstance(task, str)]
            logging.warning(f"LLM returned invalid structure for {person_name}: {data}")
            return []
        except json.JSONDecodeError as jde:
            st.error(f"è§£æ{person_name}çš„å·¥ä½œåˆ†æç»“æœå¤±è´¥ï¼ŒLLMè¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {result_content}. é”™è¯¯: {jde}")
            logging.error(f"JSON Decode Error for {person_name}: {result_content}", exc_info=True)
            return []
    except Exception as e:
        st.error(f"åˆ†æ{person_name}å·¥ä½œå‡ºé”™: {e}")
        logging.error(f"Error analyzing {person_name}'s work: {e}", exc_info=True)
        return []

@st.cache_data(show_spinner=False) # Cache the function result
def analyze_work_history(df: pd.DataFrame) -> pd.DataFrame:
    """åˆ†æå·¥ä½œå†å²ï¼Œè®°å½•æ¯é¡¹å·¥ä½œçš„é¦–æ¬¡å’Œæœ€åä¸€æ¬¡å‡ºç°"""
    # Ensure 'æäº¤æ—¶é—´' is datetime for proper sorting
    df['æäº¤æ—¶é—´'] = pd.to_datetime(df['æäº¤æ—¶é—´'], errors='coerce')
    df.dropna(subset=['å§“å', 'æäº¤æ—¶é—´', 'ä»Šæ—¥æ€»ç»“'], inplace=True) # Drop rows with missing crucial data

    work_records = []
    
    # Group by person and sort by time
    grouped = df.groupby('å§“å')
    
    # Use st.progress for better UX during analysis
    progress_bar = st.progress(0)
    total_persons = len(grouped)
    
    for i, (name, group) in enumerate(grouped):
        group = group.sort_values('æäº¤æ—¶é—´').reset_index(drop=True) # Reset index after sorting for .iloc safety
        valid_summaries = [s for s in group['ä»Šæ—¥æ€»ç»“'].tolist() if pd.notna(s) and s.strip() != '']
        
        if not valid_summaries:
            logging.info(f"Skipping {name}: No valid summaries after filtering.")
            progress_bar.progress((i + 1) / total_persons)
            continue
            
        st.info(f"æ­£åœ¨åˆ†æ: {name} çš„å·¥ä½œæ€»ç»“ ({len(valid_summaries)} æ¡)")
        keywords = extract_keywords_for_person(name, valid_summaries)
        
        if not keywords:
            st.warning(f"âš ï¸ æœªèƒ½ä¸º {name} æå–åˆ°æœ‰æ•ˆå…³é”®è¯ã€‚")
            logging.warning(f"No keywords extracted for {name}.")
            progress_bar.progress((i + 1) / total_persons)
            continue

        st.write(f"æå–åˆ°å…³é”®è¯: {', '.join(keywords)}") # Display keywords for user feedback
        
        # Associate extracted keywords with their original dates from the group
        # This part needs careful alignment. If LLM extracts fewer keywords than summaries,
        # or if it summarizes across multiple days, the simple modulo approach might not be accurate.
        # A more robust approach might be to try and map keywords back to specific summaries/dates.
        # For now, let's keep the modulo but add a check to prevent IndexError.
        for work in keywords:
            # Try to find a date that's closest or representative.
            # A simple approach: find the first summary that might contain the work, then use its date.
            # This is heuristic and might not be perfect.
            associated_date = None
            for original_summary, submit_time in zip(group['ä»Šæ—¥æ€»ç»“'], group['æäº¤æ—¶é—´']):
                if work in str(original_summary): # Check if the keyword is in the original summary
                    associated_date = submit_time
                    break
            
            if associated_date is None: # If not directly found, fallback to the group's earliest/latest date
                associated_date = group['æäº¤æ—¶é—´'].min() # Or group['æäº¤æ—¶é—´'].max()

            work_records.append({
                'work': work,
                'person': name,
                'date': associated_date # Use the derived date
            })
        
        progress_bar.progress((i + 1) / total_persons)
    
    progress_bar.empty() # Clear the progress bar after completion
    
    # Group by work content and count occurrences for each person
    work_stats_detailed = defaultdict(lambda: defaultdict(list))
    for record in work_records:
        work_stats_detailed[record['work']][record['person']].append(record['date'])
    
    # Generate summary report
    report = []
    for work, persons_data in work_stats_detailed.items():
        all_dates_for_work = []
        all_persons_involved = []
        person_occurrence_counts = defaultdict(int)

        for person, dates in persons_data.items():
            all_dates_for_work.extend(dates)
            all_persons_involved.append(person)
            person_occurrence_counts[person] += len(dates) # Count how many times this person did this work

        if not all_dates_for_work:
            continue

        # Determine the main person based on the most occurrences for this specific work
        main_person = max(person_occurrence_counts.items(), key=lambda item: item[1])[0]

        report.append({
            'å·¥ä½œå†…å®¹': work,
            'è´Ÿè´£äºº': main_person,
            'å¼€å§‹æ—¶é—´': min(all_dates_for_work).strftime('%Y-%m-%d') if pd.notna(min(all_dates_for_work)) else 'N/A',
            'ç»“æŸæ—¶é—´': max(all_dates_for_work).strftime('%Y-%m-%d') if pd.notna(max(all_dates_for_work)) else 'N/A',
            'å‡ºç°æ¬¡æ•°': len(all_dates_for_work),
            'æ¶‰åŠäººå‘˜': ', '.join(sorted(set(all_persons_involved))) # Ensure unique and sorted persons
        })
    
    # Sort the final report by 'å¼€å§‹æ—¶é—´' for better readability
    final_report_df = pd.DataFrame(report)
    if not final_report_df.empty:
        final_report_df['å¼€å§‹æ—¶é—´_sort'] = pd.to_datetime(final_report_df['å¼€å§‹æ—¶é—´'], errors='coerce')
        final_report_df = final_report_df.sort_values(by='å¼€å§‹æ—¶é—´_sort').drop(columns=['å¼€å§‹æ—¶é—´_sort'])
    
    return final_report_df

# --- Main UI logic ---

st.sidebar.header("æ–‡ä»¶ä¸Šä¼ ")
uploaded_file = st.sidebar.file_uploader("ä¸Šä¼ å·¥ä½œæ€»ç»“CSVæ–‡ä»¶", type=["csv"])

if uploaded_file:
    # Check if a new file is uploaded or if the content has changed (simple filename check)
    if uploaded_file.name != st.session_state.last_uploaded:
        st.session_state.report_df = None  # Clear previous analysis results
        st.session_state.last_uploaded = uploaded_file.name
        st.session_state.file_processed = False # Flag to indicate new file, not yet processed

    try:
        # Explicitly specify encoding for reading CSV, common for Chinese characters
        df = pd.read_csv(uploaded_file, encoding='utf-8')
        # Check if essential columns exist
        required_columns = ['å§“å', 'æäº¤æ—¶é—´', 'ä»Šæ—¥æ€»ç»“']
        if not all(col in df.columns for col in required_columns):
            st.error(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—ã€‚è¯·ç¡®ä¿åŒ…å« '{', '.join(required_columns)}'ã€‚")
            st.session_state.report_df = None
            st.session_state.file_processed = False
        else:
            st.session_state.raw_df = df # Store raw dataframe for analysis
            st.session_state.file_processed = True
            with st.expander("åŸå§‹æ•°æ®é¢„è§ˆ"):
                st.dataframe(df.head())
    except UnicodeDecodeError:
        st.error("æ— æ³•è§£ç CSVæ–‡ä»¶ã€‚è¯·å°è¯•ä½¿ç”¨UTF-8ç¼–ç çš„æ–‡ä»¶ã€‚")
        st.session_state.report_df = None
        st.session_state.file_processed = False
    except Exception as e:
        st.error(f"è¯»å–CSVæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        st.session_state.report_df = None
        st.session_state.file_processed = False

    if st.session_state.file_processed:
        if st.button("å¼€å§‹åˆ†æ", key="analyze_button"):
            with st.spinner("æ­£åœ¨åˆ†æä¸­ï¼Œè¯·ç¨å€™..."):
                # Use the raw_df from session state
                st.session_state.report_df = analyze_work_history(st.session_state.raw_df.copy()) # Pass a copy to avoid modifying original df in cache
            st.success("åˆ†æå®Œæˆï¼")

# If analysis results are available, display the report and filtering controls
if st.session_state.report_df is not None and not st.session_state.report_df.empty:
    report_df = st.session_state.report_df
    
    st.markdown("---")
    st.subheader("åˆ†æç»“æœ")
    st.dataframe(report_df, use_container_width=True) # Use container_width for better display

    st.markdown("---")
    st.subheader("æ•°æ®ç­›é€‰")
    col1, col2 = st.columns(2)
    
    with col1:
        # Get unique values for dropdowns, ensure they are string types
        unique_persons = [str(p) for p in report_df['è´Ÿè´£äºº'].unique()]
        selected_person = st.multiselect(
            "æŒ‰è´Ÿè´£äººç­›é€‰",
            options=unique_persons,
            default=unique_persons # Default to all selected
        )
        
    with col2:
        unique_works = [str(w) for w in report_df['å·¥ä½œå†…å®¹'].unique()]
        selected_work = st.multiselect(
            "æŒ‰å·¥ä½œå†…å®¹ç­›é€‰", 
            options=unique_works,
            default=unique_works # Default to all selected
        )
    
    # Apply filtering conditions
    # Check if selected_person or selected_work are empty lists (if nothing is selected)
    person_filter = report_df['è´Ÿè´£äºº'].isin(selected_person) if selected_person else pd.Series([True] * len(report_df))
    work_filter = report_df['å·¥ä½œå†…å®¹'].isin(selected_work) if selected_work else pd.Series([True] * len(report_df))
    
    filtered_df = report_df[person_filter & work_filter]
    
    if not filtered_df.empty:
        st.dataframe(filtered_df, use_container_width=True)
    else:
        st.warning("æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ•°æ®ï¼Œè¯·è°ƒæ•´ç­›é€‰æ¡ä»¶")
        
    st.download_button(
        label="ä¸‹è½½åˆ†æç»“æœ (CSV)",
        data=filtered_df.to_csv(index=False, encoding='utf-8-sig'), # 'utf-8-sig' for BOM, better for Excel
        file_name="å·¥ä½œå†å²åˆ†ææŠ¥å‘Š.csv",
        mime="text/csv"
    )
    st.download_button(
        label="ä¸‹è½½åˆ†æç»“æœ (JSON)",
        data=filtered_df.to_json(orient='records', force_ascii=False, indent=4).encode('utf-8'),
        file_name="å·¥ä½œå†å²åˆ†ææŠ¥å‘Š.json",
        mime="application/json"
    )

st.markdown("---")
with st.expander("ä½¿ç”¨è¯´æ˜", expanded=True):
    st.markdown("""
    1.  **ä¸Šä¼ CSVæ–‡ä»¶**: è¯·ä¸Šä¼ åŒ…å«å·¥ä½œæ€»ç»“çš„CSVæ–‡ä»¶ã€‚æ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰åˆ—ï¼š
        * `å§“å` (å‘˜å·¥å§“å)
        * `æäº¤æ—¶é—´` (æ—¥æŠ¥æäº¤æ—¥æœŸ/æ—¶é—´)
        * `ä»Šæ—¥æ€»ç»“` (å…·ä½“çš„å·¥ä½œæ€»ç»“å†…å®¹)
        ç¡®ä¿CSVæ–‡ä»¶é‡‡ç”¨**UTF-8ç¼–ç **ï¼Œç‰¹åˆ«æ˜¯åŒ…å«ä¸­æ–‡å†…å®¹æ—¶ã€‚
    2.  **å¼€å§‹åˆ†æ**: ä¸Šä¼ æ–‡ä»¶åï¼Œç‚¹å‡»"**å¼€å§‹åˆ†æ**"æŒ‰é’®ï¼Œç³»ç»Ÿå°†åˆ©ç”¨AIæ¨¡å‹æå–å’Œæ ‡å‡†åŒ–æ¯ä½å‘˜å·¥çš„å·¥ä½œä»»åŠ¡ã€‚è¿™ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚
    3.  **æŸ¥çœ‹å’Œç­›é€‰ç»“æœ**: åˆ†æå®Œæˆåï¼Œæ‚¨å¯ä»¥çœ‹åˆ°ä¸€ä¸ªæ±‡æ€»æŠ¥å‘Šã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä¸‹æ–¹çš„ç­›é€‰å™¨æ ¹æ®"**è´Ÿè´£äºº**"æˆ–"**å·¥ä½œå†…å®¹**"æ¥ç²¾ç»†æŸ¥çœ‹æ•°æ®ã€‚
    4.  **ä¸‹è½½æŠ¥å‘Š**: æ‚¨å¯ä»¥å°†ç­›é€‰åçš„åˆ†æç»“æœä¸‹è½½ä¸ºCSVæˆ–JSONæ ¼å¼ã€‚
    """)
    st.info("ğŸ’¡ **æç¤º**: é¦–æ¬¡è¿è¡Œæ—¶ï¼Œæˆ–ä¸Šä¼ æ–°æ–‡ä»¶åï¼Œåˆ†æè¿‡ç¨‹å¯èƒ½è¾ƒé•¿ã€‚åç»­å¦‚æœæ–‡ä»¶å†…å®¹ä¸å˜ï¼Œç³»ç»Ÿä¼šä½¿ç”¨ç¼“å­˜åŠ é€Ÿã€‚")
